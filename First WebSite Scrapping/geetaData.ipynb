{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing request module\n",
    "import requests\n",
    "import json\n",
    "#Setting UP the Project\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Start with Data Scrapping From The WebSite :\n",
    "\n",
    "https://gitajourney.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks the request getting correct\n",
    "\n",
    "base_url  = \"https://gitajourney.com\"\n",
    "response = requests.get(base_url)\n",
    "html_content = response.content\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all the pages URL exits in the site.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_element = soup.find('select', {'id': 'archives-dropdown-2'}) \n",
    "options = select_element.find_all('option')\n",
    "\n",
    "pages = [option.get('value') for option in options][1:]\n",
    "print(pages)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Post Class for post object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a post object\n",
    "class Post:\n",
    "    def __init__(self, chapter, verse, content):\n",
    "        self.verse = verse\n",
    "        self.chapter = chapter\n",
    "        self.content = content\n",
    "    def toString(self):\n",
    "        print(\"heading :\"+self.heading, \"shloka :\"+self.shloka, \"content :\"+ \",\".join(self.content))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally Scrapping Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://gitajourney.com/2013/08/', 'https://gitajourney.com/2013/07/', 'https://gitajourney.com/2013/06/', 'https://gitajourney.com/2013/05/', 'https://gitajourney.com/2013/04/', 'https://gitajourney.com/2013/03/', 'https://gitajourney.com/2013/02/', 'https://gitajourney.com/2013/01/', 'https://gitajourney.com/2012/12/', 'https://gitajourney.com/2012/11/', 'https://gitajourney.com/2012/10/', 'https://gitajourney.com/2012/09/', 'https://gitajourney.com/2012/08/', 'https://gitajourney.com/2012/07/', 'https://gitajourney.com/2012/06/', 'https://gitajourney.com/2012/05/', 'https://gitajourney.com/2012/04/', 'https://gitajourney.com/2012/03/', 'https://gitajourney.com/2012/02/', 'https://gitajourney.com/2012/01/', 'https://gitajourney.com/2011/12/', 'https://gitajourney.com/2011/11/', 'https://gitajourney.com/2011/10/', 'https://gitajourney.com/2011/09/', 'https://gitajourney.com/2011/08/']\n",
      "79\n",
      "695\n"
     ]
    }
   ],
   "source": [
    "posts =[]\n",
    "num = 0\n",
    "while num < len(pages):\n",
    "   \n",
    "    page_url = pages[num]\n",
    "\n",
    "    page_res = requests.get(page_url)\n",
    "\n",
    "    page_content = page_res.content\n",
    "\n",
    "    page_soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "    prev_page_url = page_soup.find('div', class_='nav-previous')\n",
    "\n",
    "\n",
    "    if(prev_page_url and prev_page_url.find('a')):\n",
    "        pages.insert(num+1, prev_page_url.find('a').get('href'))\n",
    "    \n",
    "\n",
    "    articles = page_soup.find_all('article', class_='post')\n",
    "   \n",
    "    for article in articles:\n",
    "        header = article.find('header', class_=\"post-title\").find('h1').find('a').text.replace('\\xa0', \" \").split(\" Chapter \")\n",
    "        \n",
    "        verse_no = header[0].split(\" \")[-1]\n",
    "        chapter_no = header[1] if 1 < len(header) else \" \"\n",
    "        content = article.find('div', class_=\"post-entry\").text.replace(\"\\xa0\", \"        \").replace(\"\\n\", \"   \").strip()\n",
    "        posts.append(Post(chapter_no, verse_no, content))\n",
    "    num = num + 1\n",
    "    \n",
    "        \n",
    "print(num)\n",
    "print(len(posts))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving to Json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data saved to 'people.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json_data = json.dumps([post.__dict__ for post in posts], indent=4)\n",
    "\n",
    "# Write JSON data to a file\n",
    "with open(\"data.json\", \"w\") as json_file:\n",
    "    json_file.write(json_data)\n",
    "\n",
    "print(\"JSON data saved to 'people.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
